#the parameter file for delineation of landforms using Deep Learning

##############################################################
## working folder and other directory setting

# working directory (current folder)
working_root = ./

##############################################################
## setting for getting training data

# get training data from study areas, for multiple areas, seperate them by using comma (,)
training_regions = study_area_1.ini

# the sub images for traing (relative path in the current folder)
# because somewhere in "datasets/get_subImages.py", we write "subImages" and "subLabels",
# we should not change values of input_train_dir and input_label_dir
input_train_dir= subImages
# the sub label images for training (relative path in the current folder)
input_label_dir= subLabels

# how many processes for getting sub-images, splitting sub-images and data augmentation
# don't set this too large because it has a lot IO operation
process_num = 4

#buffer size for extending the training polygon, in the XY projection, normally, it is based on meters
buffer_size = 300

#whether use the rectangular extent of the polygon, set "--rectangle" on right if Yes, or omit it if NO
b_use_rectangle = --rectangle

#the nodata in output images, for sentinel, set dst_nodata as 0
dst_nodata = 0

# image format for splitting images: .tif or .png
split_image_format = .png

# the percentage of trainig data, the remaining are for validation
training_data_per = 0.9
# the interval (epoch) for running evaluation on validation dataset. #if this is not set, it will only run once after training
# removed, now we conduct validation for every 1200 seconds (default)
# validation_interval = 1

# some pre-trained model was trained with 19 class (Cityscapes) or 21 classes (PASCAL VOC 2012), if yes, the model will try
# output 19 or 21 classes, but those classes without training data are nan.
b_initialize_last_layer = yes

# if the overall miou does not improve (< 0.0001) for five consecutive evaluation (epochs) on validation data, we stop training
b_early_stopping = Yes

# txt containing list for training and validation
# if set dataset_name as cityscapes, pascal_voc_seg, or ade20k, num_classes and ignore_label will use the one in 'data_generator.py'
dataset_name = thawslump_planet
training_sample_list_txt = train_list.txt
validation_sample_list_txt = val_list.txt

# image crop size (height, width) for the DeepLab model. Patch size should not greater than this
image_crop_size = 480, 480

## patch width and height of training images (eg. 480=160+160*2)
train_patch_width = 160
train_patch_height = 160
train_pixel_overlay_x = 160
train_pixel_overlay_y = 160

# data augmentation # flip, rotate, blur, crop, scale, bright, contrast, noise
data_augmentation = flip, rotate, blur, crop, scale
# ignore class when perform data augmentation, multiple class will be support in future
data_aug_ignore_classes = class_0

# class number (without background)
NUM_CLASSES_noBG = 1

##############################################################
## deep learning setting
# experiment name
expr_name=exp3
# network setting files
network_setting_ini = deeplabv3plus_xception71.ini

##############################################################
## setting for inference (prediction)

# study areas for inference (prediction), for multiple areas, seperate them by using comma (,)
inference_regions = study_area_1.ini

# output folder for inference results
inf_output_dir = multi_inf_results

# indicate weather to use multiple available GPUs or only use one GPU (CPU)
b_use_multiGPUs = Yes


# the expected width of patch (70)
inf_patch_width= 160
# the expected height of patch (70)
inf_patch_height=160
# the overlay of patch in pixel (210)
inf_pixel_overlay_x=160
inf_pixel_overlay_y=160


##############################################################
### Post processing and evaluation Parameters

# the minimum area of gully or other landforms, if any polygon small than minimum_area, it will be removed
minimum_area = 900

# assuming ratio=height/width (suppose height > width), ratio belong to [0,1], if any polygon has ratio greater than
# maximum_ratio_width_height, it will be removed
maximum_ratio_width_height = 1.0

# the more narrow, the ratio (=perimeter^2/area) is larger
minimum_ratio_perimeter_area = 0

# the minimum mean elevation in meters (if dem_mean of a polyogn small than this value, it will be removed)
minimum_elevation =

# the polygon with mean slope small than minimum_slope and greater than maximum_slope will be removed
minimum_slope =
maximum_slope =

# only count the pixel within this range when do statistics
dem_difference_range = None, -0.5
# expand the polygon when doing dem difference statistics (meters)
buffer_size_dem_diff = 50
# the minimum area of total elevation reduction calculated by the two parameter above (m^2)
minimum_dem_reduction_area = 4

# in meters, for some mapped polygons close to each other, the connected parts are narrow,
#we may remove these narrow parts (mapped_polygon_narrow_threshold*2), then separate them, some smaller polygons may also be removed in this way
mapped_polygon_narrow_threshold =

# indicate whether use the surrounding buffer area to calcuate the topography information, if NO, it counts the pixels inside a polygon
b_topo_use_buffer_area = NO

# indicate whether to calculate the shape information (ratio_p_a, circularit, etc.)
b_calculate_shape_info = NO

# keep holes
b_keep_holes = YES

IOU_threshold = 0.5

# if multiple observation available, remove polygons if at the same location, the occurrence of polygons is smaller than this threshold
threshold_occurrence_multi_observation = 2

# indicate whether remove false positive by utlizing multi-temporal mapping results (polygon-based analysis)
b_remove_polygons_using_multitemporal_results = NO

##############################################################
