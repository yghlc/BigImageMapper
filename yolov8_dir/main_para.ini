#the parameter file for delineation of landforms using Deep Learning

##############################################################
## working folder and other directory setting

# working directory (current folder)
working_root = ./

##############################################################
## setting for getting training data

# get training data from study areas, for multiple areas, seperate them by using comma (,)
training_regions = s2_area_inis/area_train_set01_s2_2024_v1.ini
#training_regions = area_Willow_River.ini,area_Banks_east.ini,area_Ellesmere_Island_nirGB.ini

# the sub images for traing (relative path in the current folder)
# because somewhere in "datasets/get_subImages.py", we write "subImages" and "subLabels",
# we should not change values of input_train_dir and input_label_dir
input_train_dir= subImages
# the sub label images for training (relative path in the current folder)
input_label_dir= subLabels

# how many processes for getting sub-images, splitting sub-images and data augmentation
# don't set this too large because it has a lot IO operation
process_num = 4

#buffer size for extending the training polygon, in the XY projection, normally, it is based on meters
train_buffer_size = 0

#whether use the rectangular extent of the polygon, set "--rectangle" on right if Yes, or omit it if NO
b_use_rectangle = --rectangle

# if set, the file name of sub-images will contain the original image name, not always the first image file name in the list (if have multiple images)
b_keep_org_file_name = Yes

# for object detection, we don't need label images
#b_no_label_image = Yes

# ignore objects that touch the image in the label
b_ignore_edge_objects = Yes

#the nodata in output images, for sentinel, set dst_nodata as 0
dst_nodata = 0

# image format for splitting images: .tif or .png
split_image_format = .png

# the percentage of trainig data, the remaining are for validation
training_data_per = 0.9

# some pre-trained model was trained with 19 class (Cityscapes) or 21 classes (PASCAL VOC 2012), if yes, the model will try
# output 19 or 21 classes, but those classes without training data are nan.
b_initialize_last_layer = yes

# if the overall miou does not improve (< 0.0001) for five consecutive evaluation (epochs) on validation data, we stop training
b_early_stopping = yes

object_names = RTS

# txt containing list for training and validation
# if set dataset_name as cityscapes, pascal_voc_seg, or ade20k, num_classes and ignore_label will use the one in 'data_generator.py'
dataset_name = thawslump_s2_rgb
training_sample_list_txt = train_list.txt
validation_sample_list_txt = val_list.txt

# image crop size (height, width) for the DeepLab model. Patch size should not greater than this
#image_crop_size = 480,480

## patch width and height of training images (eg. 480=160+160*2)
train_patch_width = 480
train_patch_height = 480
train_pixel_overlay_x = 0
train_pixel_overlay_y = 0

# data augmentation
#data_augmentation = flip, rotate, blur, crop, scale, bright, contrast, noise
data_augmentation =  
# ignore class when perform data augmentation, multiple class will be support in future
data_aug_ignore_classes = class_0

# class number (without background)
NUM_CLASSES_noBG = 1

##############################################################
## deep learning setting
# experiment name
expr_name=exp2
# network setting files
network_setting_ini = yolo_model.ini

##############################################################
## setting for inference (prediction)

# study areas for inference (prediction), for multiple areas, seperate them by using comma (,)
#inference_regions = s2_area_inis/area_Dai2025_2024_RGB.ini
#inference_regions = s2_area_inis/area_panArctic_2024_RGB.ini
inference_regions = s2_area_inis/area_train_set01_s2_2024_v1.ini

# output folder for inference results
inf_output_dir = multi_inf_results

# indicate weather to use multiple available GPUs or only use one GPU (CPU)
b_use_multiGPUs = Yes

# maximum simultaneous jobs for prediction (although there is enough GPU memory, but may don't have enough CPU memory for loading data)
maximum_prediction_jobs = 7

# the expected width of patch (70)
inf_patch_width= 320
# the expected height of patch (70)
inf_patch_height=320
# the overlay of patch in pixel (210)
inf_pixel_overlay_x=80
inf_pixel_overlay_y=80

# using python API when do the prediction
b_inf_use_python_api = Yes

# the batch size for prediction
inf_batch_size = 16


##############################################################
### Post processing and evaluation Parameters

# minimum IOU overlap to retain when non_max_suppression (small value will remove more boxes)
nms_overlapThresh = 0.5

# assuming ratio=height/width (suppose height > width), ratio belong to [0,1], if any box has ratio less than
# minimum_ratio_width_height, it will be removed
minimum_ratio_width_height = 0.1

# the minimum area of gully or other landforms, if any polygon small than minimum_area, it will be removed
#minimum_area = 900

# the more narrow, the ratio (=perimeter^2/area) is larger
minimum_ratio_perimeter_area = 0

# the minimum mean elevation in meters (if dem_mean of a polyogn small than this value, it will be removed)
#minimum_elevation =

# the polygon with mean slope small than minimum_slope and greater than maximum_slope will be removed
#minimum_slope = 1
#maximum_slope =

# only count the pixel within this range when do statistics (the unit is cm)
#dem_difference_range = None, -200
# expand the polygon when doing dem difference statistics (meters)
#buffer_size_dem_diff = 50

# the minimum area of total elevation reduction calculated by the two parameter above (m^2)
#minimum_dem_reduction_area = 4

# in meters, for some mapped polygons close to each other, the connected parts are narrow,
#we may remove these narrow parts (mapped_polygon_narrow_threshold*2), then separate them, some smaller polygons may also be removed in this way
#mapped_polygon_narrow_threshold =

# indicate whether use the surrounding buffer area to calcuate the topography information, if NO, it counts the pixels inside a polygon
b_topo_use_buffer_area = NO

# indicate whether to calculate the shape information (ratio_p_a, circularit, etc.)
#b_calculate_shape_info = NO

# keep holes
#b_keep_holes = YES

#IOU_threshold = 0.1

# if multiple observation available, remove polygons if at the same location, the occurrence of polygons is smaller than this threshold
#threshold_occurrence_multi_observation = 1

# indicate whether remove false positive by utlizing multi-temporal mapping results (polygon-based analysis)
#b_remove_polygons_using_multitemporal_results = NO

##############################################################
